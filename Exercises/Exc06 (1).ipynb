{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***Data Scraping***"
      ],
      "metadata": {
        "id": "Ofn0xOIETHuv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lj9vPCOz808b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad1dd481-cc5a-4982-da00-2f5722c8aa26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def fetch_page(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        return soup\n",
        "    else:\n",
        "        return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Data Indexing***"
      ],
      "metadata": {
        "id": "TrLOkVjv_o_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def index_words(soup):\n",
        "  index = {} # a dictionary\n",
        "  words = re.findall(r'\\w+', soup.get_text())\n",
        "  for word in words:\n",
        "    word = word.lower()\n",
        "    if word in index:\n",
        "      index[word] += 1\n",
        "    else:\n",
        "      index[word] = 1\n",
        "  return index\n"
      ],
      "metadata": {
        "id": "GfEZw55t-2qu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Text Processing***"
      ],
      "metadata": {
        "id": "oQpdS3wo_vPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words(index):\n",
        "  stop_words = {'a', 'an', 'the', 'and', 'or', 'in', 'on', 'at'}\n",
        "  for stop_word in stop_words:\n",
        "    if stop_word in index:\n",
        "      del index[stop_word]\n",
        "  return index\n"
      ],
      "metadata": {
        "id": "oX8Sc03p_rt5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def apply_stemming(index):\n",
        "  stemmer = PorterStemmer()\n",
        "  stemmed_index = {}\n",
        "  for word, count in index.items():\n",
        "    stemmed_word = stemmer.stem(word)\n",
        "    if stemmed_word in stemmed_index:\n",
        "      stemmed_index[stemmed_word] += count\n",
        "    else:\n",
        "      stemmed_index[stemmed_word] = count\n",
        "  return stemmed_index\n"
      ],
      "metadata": {
        "id": "aciC1ufiAFEO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Search Query***"
      ],
      "metadata": {
        "id": "hxTqjvYCBKWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search(query, index):\n",
        "  query_words = re.findall(r'\\w+', query.lower())\n",
        "  print(f\"Query: {query_words}\")\n",
        "  results = {}\n",
        "  for word in query_words:\n",
        "    if word in index:\n",
        "      results[word] = index[word]\n",
        "  return results\n"
      ],
      "metadata": {
        "id": "Cs_mbOZnBNYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search2(query, index):\n",
        "  stemmer = PorterStemmer()\n",
        "  query_words = re.findall(r'\\w+', query.lower())\n",
        "  results = {}\n",
        "  for word in query_words:\n",
        "    word = stemmer.stem(word)\n",
        "    if word in index:\n",
        "      results[word] = index[word]\n",
        "  return results\n"
      ],
      "metadata": {
        "id": "rG1DEsoeDYUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Full search engine***"
      ],
      "metadata": {
        "id": "Ase8q1xJB7X2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_engine(url, query):\n",
        "  soup = fetch_page(url)\n",
        "  if soup is None:\n",
        "    return None\n",
        "  index = index_words(soup)\n",
        "  index = remove_stop_words(index)\n",
        "  index = apply_stemming(index)\n",
        "  results = search(query, index)\n",
        "  return results\n"
      ],
      "metadata": {
        "id": "0PNRqm8vB-8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_engine2(url, query):\n",
        "  soup = fetch_page(url)\n",
        "  if soup is None:\n",
        "    return None\n",
        "  index = index_words(soup)\n",
        "  index = remove_stop_words(index)\n",
        "  index = apply_stemming(index)\n",
        "  results = search2(query, index)\n",
        "  return results\n"
      ],
      "metadata": {
        "id": "RptlLfirDwjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Braude***"
      ],
      "metadata": {
        "id": "0dgkdqcEF65w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://w3.braude.ac.il/?lang=en'\n",
        "query = 'Industry'\n",
        "results = search_engine2(url, query)\n",
        "print(results)\n",
        "rank=1\n",
        "for word, count in results.items():\n",
        "  rank = rank*1/count\n",
        "rank = 1-rank\n",
        "print(rank)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLxky85kF9YI",
        "outputId": "3aae02b7-baec-48fa-b594-c34ccdd3cf05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'industri': 8}\n",
            "0.875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://w3.braude.ac.il/?lang=en'\n",
        "query = 'Braude college'\n",
        "results = search_engine2(url, query)\n",
        "print(results)\n",
        "rank=1\n",
        "for word, count in results.items():\n",
        "  rank = rank*1/count\n",
        "rank = 1-rank\n",
        "print(rank)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk_iKlzzGN16",
        "outputId": "9c37b002-87ba-45a3-bc50-510b759f22f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'braud': 13, 'colleg': 8}\n",
            "0.9903846153846154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://w3.braude.ac.il/?lang=en'\n",
        "query = 'Galilee center'\n",
        "results = search_engine2(url, query)\n",
        "print(results)\n",
        "rank=1\n",
        "for word, count in results.items():\n",
        "  rank = rank*1/count\n",
        "rank = 1-rank\n",
        "print(rank)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7dDC4Q3GTgF",
        "outputId": "3101abc2-b08e-49f1-8bac-2170d6ce1f63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'galile': 15, 'center': 4}\n",
            "0.9833333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Search engine for more than one page***"
      ],
      "metadata": {
        "id": "ScMTdavHGZsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from collections import defaultdict\n",
        "\n",
        "class WikiSearchEngine:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the search engine\"\"\"\n",
        "        self.base_url = \"https://en.wikipedia.org/w/api.php\"\n",
        "        self.pages = []\n",
        "        self.word_locations = defaultdict(list)  # word -> [(page_id, frequency), ...]\n",
        "        self.stop_words = {'a', 'an', 'the', 'and', 'or', 'in', 'on', 'at', 'to', 'for', 'of', 'with'}\n",
        "\n",
        "    def fetch_wiki_pages(self, topic, num_pages=5):\n",
        "        \"\"\"Fetch Wikipedia pages for given topic\"\"\"\n",
        "        search_params = { # This is a dictionary that defines the parameters for the search request sent to Wikipedia:\n",
        "            \"action\": \"query\", # This indicates that we're performing a query action.\n",
        "            \"format\": \"json\", # The response will be in JSON format.\n",
        "            \"list\": \"search\", # Specifies that we want to perform a search.\n",
        "            \"srsearch\": topic, # This is the search term based on the topic parameter.\n",
        "            \"srlimit\": num_pages  # Limit to the first num_pages results\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Send the request to Wikipedia API\n",
        "            response = requests.get(self.base_url, params=search_params)\n",
        "            # extract the search results (response.json()['query']['search']), which include basic information such as pageid and title.\n",
        "            search_results = response.json()['query']['search']\n",
        "\n",
        "            # For each search result, fetch the content of the page\n",
        "            for result in search_results:\n",
        "                content_params = {\n",
        "                    \"action\": \"query\",\n",
        "                    \"format\": \"json\",\n",
        "                    \"prop\": \"extracts|info\", # To get both plain text extracts and metadata (e.g., page URL).\n",
        "                    \"pageids\": result['pageid'], # To get the URL of the page.\n",
        "                    \"inprop\": \"url\",\n",
        "                    \"explaintext\": True  # Get plain text extract\n",
        "                }\n",
        "                content_response = requests.get(self.base_url, params=content_params)\n",
        "                page_data = content_response.json()['query']['pages'][str(result['pageid'])]\n",
        "\n",
        "                # Append the page info to self.pages\n",
        "                self.pages.append({\n",
        "                    'id': result['pageid'],\n",
        "                    'title': page_data['title'],\n",
        "                    'url': page_data.get('fullurl', f\"https://en.wikipedia.org/?curid={result['pageid']}\"),\n",
        "                    'content': page_data['extract']\n",
        "                })\n",
        "                print(f\"Retrieved: {page_data['title']}\")\n",
        "\n",
        "            # Return the fetched pages after all are retrieved\n",
        "            return self.pages\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching pages: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def build_index(self):\n",
        "        \"\"\"Build a simple word location index\"\"\"\n",
        "        self.word_locations.clear()  # Clear any existing index\n",
        "\n",
        "        # Process each page in the stored pages\n",
        "        for page in self.pages:\n",
        "            # Get all words from the content of the page\n",
        "            words = re.findall(r'\\w+', page['content'].lower())  # Extract words and make them lowercase\n",
        "\n",
        "            # Count word frequencies (excluding stop words)\n",
        "            word_counts = defaultdict(int) # whenever you access a key that hasn’t been added yet, it will automatically be assigned the value 0.\n",
        "            for word in words:\n",
        "                if word not in self.stop_words:\n",
        "                    word_counts[word] += 1\n",
        "\n",
        "            # Add the word counts to the index with page information\n",
        "            for word, count in word_counts.items():\n",
        "                self.word_locations[word].append((page['id'], count))\n",
        "\n",
        "    # Example method to display the index (for testing purposes)\n",
        "    def display_index(self):\n",
        "        for word, locations in self.word_locations.items():\n",
        "            print(f\"Word: {word}\")\n",
        "            for page_id, count in locations:\n",
        "                print(f\"  Page ID: {page_id}, Count: {count}\")\n",
        "\n",
        "    def search(self, query, num_results=5):\n",
        "      \"\"\" Search pages using simple word frequency ranking.\n",
        "      Ranks pages based on:1. Number of query words found in the page\n",
        "      2. Total frequency of query words \"\"\"\n",
        "\n",
        "      # Get query words\n",
        "      query_words = [word.lower() for word in re.findall(r'\\w+', query)\n",
        "      if word.lower() not in self.stop_words]\n",
        "      if not query_words:\n",
        "        return []\n",
        "      # Calculate scores for each page\n",
        "      page_scores = defaultdict(lambda: {'matches': 0, 'total_freq': 0})\n",
        "      # For each query word\n",
        "      for word in query_words:\n",
        "        # Find pages containing this word\n",
        "        for page_id, freq in self.word_locations.get(word, []):\n",
        "          page_scores[page_id]['matches'] += 1\n",
        "          page_scores[page_id]['total_freq'] += freq\n",
        "\n",
        "      # Convert to list and sort\n",
        "      ranked_results = [\n",
        "        (page_id, scores['matches'], scores['total_freq'])\n",
        "        for page_id, scores in page_scores.items()\n",
        "      ]\n",
        "\n",
        "      # Sort by number of matching words first, then by total frequency\n",
        "      ranked_results.sort(key=lambda x: (x[1], x[2]), reverse=True)\n",
        "\n",
        "      # Format results\n",
        "      results = []\n",
        "      for page_id, matches, total_freq in ranked_results[:num_results]:\n",
        "        page = next(p for p in self.pages if p['id'] == page_id)\n",
        "        # Find the first matching word context\n",
        "        context = self.get_context(page['content'], query_words)\n",
        "        results.append({\n",
        "          'title': page['title'],\n",
        "          'url': page['url'],\n",
        "          'matching_words': matches,\n",
        "          'total_frequency': total_freq,\n",
        "          'context': context\n",
        "          })\n",
        "\n",
        "      return results\n",
        "\n"
      ],
      "metadata": {
        "id": "HdFxf9ALGhuZ",
        "collapsed": true
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "engine = WikiSearchEngine()\n",
        "\n",
        "# Fetch Wikipedia pages related to \"Bird\"\n",
        "pages = engine.fetch_wiki_pages(\"Bird\", num_pages=3)\n",
        "\n",
        "# Print out the titles and URLs of the fetched pages\n",
        "if pages:\n",
        "    for page in pages:\n",
        "        print(f\"Title: {page['title']}\")\n",
        "        print(f\"URL: {page['url']}\")\n",
        "        print(f\"Content: {page['content'][:300]}...\")  # Print first 300 characters of content\n",
        "        print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "z_Ukfcz_XGjD",
        "outputId": "1e098b79-81ea-4a02-ecac-71c3cebe014a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved: Bird\n",
            "Retrieved: Bird (disambiguation)\n",
            "Retrieved: Bird & Bird\n",
            "Title: Bird\n",
            "URL: https://en.wikipedia.org/wiki/Bird\n",
            "Content: Birds are a group of warm-blooded vertebrates constituting the class Aves (Latin: [ˈaveːs]), characterised by feathers, toothless beaked jaws, the laying of hard-shelled eggs, a high metabolic rate, a four-chambered heart, and a strong yet lightweight skeleton. Birds live worldwide and range in size...\n",
            "\n",
            "\n",
            "Title: Bird (disambiguation)\n",
            "URL: https://en.wikipedia.org/wiki/Bird_(disambiguation)\n",
            "Content: A bird is a feathered, winged, bipedal, warm-blooded, egg-laying, vertebrate.\n",
            "Bird, BIRD, or the bird may also refer to:\n",
            "\n",
            "\n",
            "== Arts and entertainment ==\n",
            "\n",
            "\n",
            "=== Fictional characters ===\n",
            "Tracy \"Bird\" Van Adams, in the Soul Food film and TV series\n",
            "Bird, of the Barksdale Organization in TV series The Wire...\n",
            "\n",
            "\n",
            "Title: Bird & Bird\n",
            "URL: https://en.wikipedia.org/wiki/Bird_%26_Bird\n",
            "Content: Bird & Bird is an international law firm that was founded in London in 1846. The firm has since expanded to over 31 offices in Europe, Asia, and the Middle East, and has a particular focus on the technology, media, and telecommunications sectors. In addition to its core practice areas of intellectua...\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}